{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-23T19:18:57.314377Z","iopub.execute_input":"2024-03-23T19:18:57.314672Z","iopub.status.idle":"2024-03-23T19:18:59.239735Z","shell.execute_reply.started":"2024-03-23T19:18:57.314646Z","shell.execute_reply":"2024-03-23T19:18:59.238511Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:02.840138Z","iopub.execute_input":"2024-03-23T19:19:02.841044Z","iopub.status.idle":"2024-03-23T19:19:03.215960Z","shell.execute_reply.started":"2024-03-23T19:19:02.841006Z","shell.execute_reply":"2024-03-23T19:19:03.215140Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"notebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:03.802591Z","iopub.execute_input":"2024-03-23T19:19:03.803313Z","iopub.status.idle":"2024-03-23T19:19:03.826229Z","shell.execute_reply.started":"2024-03-23T19:19:03.803277Z","shell.execute_reply":"2024-03-23T19:19:03.825264Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"567dc1aba9854d37ab1e646f3f69310e"}},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:15.400031Z","iopub.execute_input":"2024-03-23T19:19:15.400855Z","iopub.status.idle":"2024-03-23T19:19:17.128228Z","shell.execute_reply.started":"2024-03-23T19:19:15.400821Z","shell.execute_reply":"2024-03-23T19:19:17.127168Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment\n0  One of the other reviewers has mentioned that ...  positive\n1  A wonderful little production. <br /><br />The...  positive\n2  I thought this was a wonderful way to spend ti...  positive\n3  Basically there's a family where a little boy ...  negative\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df['Category'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:17.130551Z","iopub.execute_input":"2024-03-23T19:19:17.131340Z","iopub.status.idle":"2024-03-23T19:19:17.186838Z","shell.execute_reply.started":"2024-03-23T19:19:17.131298Z","shell.execute_reply":"2024-03-23T19:19:17.186032Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  Category\n0  One of the other reviewers has mentioned that ...  positive         1\n1  A wonderful little production. <br /><br />The...  positive         1\n2  I thought this was a wonderful way to spend ti...  positive         1\n3  Basically there's a family where a little boy ...  negative         0\n4  Petter Mattei's \"Love in the Time of Money\" is...  positive         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:17.188139Z","iopub.execute_input":"2024-03-23T19:19:17.188515Z","iopub.status.idle":"2024-03-23T19:19:17.201344Z","shell.execute_reply.started":"2024-03-23T19:19:17.188486Z","shell.execute_reply":"2024-03-23T19:19:17.200382Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(50000, 3)"},"metadata":{}}]},{"cell_type":"code","source":"df = df.rename(columns={\n    'review': 'text',\n    'Category': 'label'\n})\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:17.203666Z","iopub.execute_input":"2024-03-23T19:19:17.204055Z","iopub.status.idle":"2024-03-23T19:19:17.211882Z","shell.execute_reply.started":"2024-03-23T19:19:17.204021Z","shell.execute_reply":"2024-03-23T19:19:17.210920Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:17.213181Z","iopub.execute_input":"2024-03-23T19:19:17.213801Z","iopub.status.idle":"2024-03-23T19:19:17.234225Z","shell.execute_reply.started":"2024-03-23T19:19:17.213766Z","shell.execute_reply":"2024-03-23T19:19:17.233281Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n1    25000\n0    25000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:17.311696Z","iopub.execute_input":"2024-03-23T19:19:17.312050Z","iopub.status.idle":"2024-03-23T19:19:19.651371Z","shell.execute_reply.started":"2024-03-23T19:19:17.312016Z","shell.execute_reply":"2024-03-23T19:19:19.650414Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_train, X_test = train_test_split(df, test_size = 0.2)\nX_train, X_val = train_test_split(X_train, test_size = 0.2) ","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:19.653261Z","iopub.execute_input":"2024-03-23T19:19:19.653740Z","iopub.status.idle":"2024-03-23T19:19:19.681418Z","shell.execute_reply.started":"2024-03-23T19:19:19.653711Z","shell.execute_reply":"2024-03-23T19:19:19.680606Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(X_train.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:19.682632Z","iopub.execute_input":"2024-03-23T19:19:19.682946Z","iopub.status.idle":"2024-03-23T19:19:19.690144Z","shell.execute_reply.started":"2024-03-23T19:19:19.682918Z","shell.execute_reply":"2024-03-23T19:19:19.689048Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"                                                    text sentiment  label\n8898   Oh my... bad clothing, worse synth music and t...  negative      0\n32429  I tend to be inclined towards movies about peo...  positive      1\n28465  What is the deal with all these ethnic crime g...  negative      0\n15621  This was the first televised episode of the Co...  positive      1\n32909  Strange, often effective hippie zombie flick, ...  negative      0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:19.692341Z","iopub.execute_input":"2024-03-23T19:19:19.692669Z","iopub.status.idle":"2024-03-23T19:19:20.782824Z","shell.execute_reply.started":"2024-03-23T19:19:19.692642Z","shell.execute_reply":"2024-03-23T19:19:20.781957Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_ds = Dataset.from_pandas(X_train, split=\"train\")\ntest_ds = Dataset.from_pandas(X_val, split = 'test')\nval_ds = Dataset.from_pandas(X_test, split=\"test\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:20.783859Z","iopub.execute_input":"2024-03-23T19:19:20.784298Z","iopub.status.idle":"2024-03-23T19:19:20.985292Z","shell.execute_reply.started":"2024-03-23T19:19:20.784272Z","shell.execute_reply":"2024-03-23T19:19:20.984337Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:20.986496Z","iopub.execute_input":"2024-03-23T19:19:20.986822Z","iopub.status.idle":"2024-03-23T19:19:31.724279Z","shell.execute_reply.started":"2024-03-23T19:19:20.986796Z","shell.execute_reply":"2024-03-23T19:19:31.723286Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:31.727388Z","iopub.execute_input":"2024-03-23T19:19:31.727948Z","iopub.status.idle":"2024-03-23T19:19:33.185538Z","shell.execute_reply.started":"2024-03-23T19:19:31.727918Z","shell.execute_reply":"2024-03-23T19:19:33.184376Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce083832fc1641f5b92583f36c894afc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e85252ffde4671ac5ccb9b1b59b595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec69d1a685e4f989dc03d7eb7b1f3be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c754875fc4947b19f17408fd1942180"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e23e7d30b94aaf97663182149f2b0a"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:33.186933Z","iopub.execute_input":"2024-03-23T19:19:33.187266Z","iopub.status.idle":"2024-03-23T19:19:33.191900Z","shell.execute_reply.started":"2024-03-23T19:19:33.187239Z","shell.execute_reply":"2024-03-23T19:19:33.190870Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"tokenized_train = train_ds.map(preprocess_function, batched=True)\ntokenized_val = val_ds.map(preprocess_function, batched=True)\ntokenized_test = test_ds.map(preprocess_function, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:33.193028Z","iopub.execute_input":"2024-03-23T19:19:33.193806Z","iopub.status.idle":"2024-03-23T19:19:57.856179Z","shell.execute_reply.started":"2024-03-23T19:19:33.193768Z","shell.execute_reply":"2024-03-23T19:19:57.855070Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb813b59bd034f7aa2a9fd848b838adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9742c89e170f41f6a7ffdf83e653817e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"583fdf0f67824697a079c424e6c73ea7"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:19:57.857725Z","iopub.execute_input":"2024-03-23T19:19:57.858094Z","iopub.status.idle":"2024-03-23T19:20:13.381011Z","shell.execute_reply.started":"2024-03-23T19:19:57.858066Z","shell.execute_reply":"2024-03-23T19:20:13.380133Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2024-03-23 19:20:01.425340: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-23 19:20:01.425477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-23 19:20:01.698603: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:13.382215Z","iopub.execute_input":"2024-03-23T19:20:13.382830Z","iopub.status.idle":"2024-03-23T19:20:13.387426Z","shell.execute_reply.started":"2024-03-23T19:20:13.382800Z","shell.execute_reply":"2024-03-23T19:20:13.386300Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:13.388876Z","iopub.execute_input":"2024-03-23T19:20:13.389291Z","iopub.status.idle":"2024-03-23T19:20:29.731166Z","shell.execute_reply.started":"2024-03-23T19:20:13.389256Z","shell.execute_reply":"2024-03-23T19:20:29.730144Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m947.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:29.732730Z","iopub.execute_input":"2024-03-23T19:20:29.733046Z","iopub.status.idle":"2024-03-23T19:20:34.603328Z","shell.execute_reply.started":"2024-03-23T19:20:29.733017Z","shell.execute_reply":"2024-03-23T19:20:34.602360Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"accuracy = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:22:09.964063Z","iopub.execute_input":"2024-03-23T19:22:09.964994Z","iopub.status.idle":"2024-03-23T19:22:10.483509Z","shell.execute_reply.started":"2024-03-23T19:22:09.964959Z","shell.execute_reply":"2024-03-23T19:22:10.482461Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0fbd26a7f06453ebbdffc5f88569b47"}},"metadata":{}}]},{"cell_type":"code","source":"id2label = {0: \"negative\", 1: \"positive\"}\nlabel2id = {\"negative\": 0, \"positive\": 1}","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:34.604575Z","iopub.execute_input":"2024-03-23T19:20:34.604880Z","iopub.status.idle":"2024-03-23T19:20:34.609557Z","shell.execute_reply.started":"2024-03-23T19:20:34.604853Z","shell.execute_reply":"2024-03-23T19:20:34.608663Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:47.111150Z","iopub.execute_input":"2024-03-23T19:20:47.111798Z","iopub.status.idle":"2024-03-23T19:20:47.116647Z","shell.execute_reply.started":"2024-03-23T19:20:47.111763Z","shell.execute_reply":"2024-03-23T19:20:47.115490Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:41:59.290154Z","iopub.execute_input":"2024-03-23T20:41:59.290853Z","iopub.status.idle":"2024-03-23T20:41:59.690351Z","shell.execute_reply.started":"2024-03-23T20:41:59.290818Z","shell.execute_reply":"2024-03-23T20:41:59.689270Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"imdbreviews_classification_roberta_v01\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:41:59.793578Z","iopub.execute_input":"2024-03-23T20:41:59.794188Z","iopub.status.idle":"2024-03-23T20:41:59.807102Z","shell.execute_reply.started":"2024-03-23T20:41:59.794156Z","shell.execute_reply":"2024-03-23T20:41:59.805820Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:42:00.542102Z","iopub.execute_input":"2024-03-23T20:42:00.542818Z","iopub.status.idle":"2024-03-23T20:42:00.746663Z","shell.execute_reply.started":"2024-03-23T20:42:00.542779Z","shell.execute_reply":"2024-03-23T20:42:00.745471Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:42:08.294958Z","iopub.execute_input":"2024-03-23T20:42:08.295772Z","iopub.status.idle":"2024-03-23T21:41:53.740259Z","shell.execute_reply.started":"2024-03-23T20:42:08.295734Z","shell.execute_reply":"2024-03-23T21:41:53.738969Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 59:32, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.168100</td>\n      <td>0.157479</td>\n      <td>0.945200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.097900</td>\n      <td>0.163266</td>\n      <td>0.953200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.1499083728790283, metrics={'train_runtime': 3574.4232, 'train_samples_per_second': 17.905, 'train_steps_per_second': 0.56, 'total_flos': 1.682412665732544e+16, 'train_loss': 0.1499083728790283, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=2, id2label=id2label, label2id=label2id)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:16:45.253755Z","iopub.execute_input":"2024-03-23T20:16:45.254631Z","iopub.status.idle":"2024-03-23T20:16:45.655231Z","shell.execute_reply.started":"2024-03-23T20:16:45.254588Z","shell.execute_reply":"2024-03-23T20:16:45.654148Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Unfreeze the classifier layer\nfor param in model.classifier.parameters():\n    param.requires_grad = True","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:16:45.822748Z","iopub.execute_input":"2024-03-23T20:16:45.823241Z","iopub.status.idle":"2024-03-23T20:16:45.831521Z","shell.execute_reply.started":"2024-03-23T20:16:45.823199Z","shell.execute_reply":"2024-03-23T20:16:45.830336Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:16:46.749522Z","iopub.execute_input":"2024-03-23T20:16:46.749913Z","iopub.status.idle":"2024-03-23T20:16:46.764262Z","shell.execute_reply.started":"2024-03-23T20:16:46.749882Z","shell.execute_reply":"2024-03-23T20:16:46.762200Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"imdbreviews_classification_roberta_v01_clf_finetuning\",\n    learning_rate=1e-4,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:16:50.557933Z","iopub.execute_input":"2024-03-23T20:16:50.558339Z","iopub.status.idle":"2024-03-23T20:16:50.569174Z","shell.execute_reply.started":"2024-03-23T20:16:50.558307Z","shell.execute_reply":"2024-03-23T20:16:50.568048Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:17:03.503594Z","iopub.execute_input":"2024-03-23T20:17:03.504307Z","iopub.status.idle":"2024-03-23T20:17:03.722190Z","shell.execute_reply.started":"2024-03-23T20:17:03.504269Z","shell.execute_reply":"2024-03-23T20:17:03.720852Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T20:17:05.376615Z","iopub.execute_input":"2024-03-23T20:17:05.377335Z","iopub.status.idle":"2024-03-23T20:31:04.242513Z","shell.execute_reply.started":"2024-03-23T20:17:05.377300Z","shell.execute_reply":"2024-03-23T20:31:04.241191Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 13:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.540600</td>\n      <td>0.512306</td>\n      <td>0.843600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.5854497680664063, metrics={'train_runtime': 822.6875, 'train_samples_per_second': 38.897, 'train_steps_per_second': 1.216, 'total_flos': 8411561773213440.0, 'train_loss': 0.5854497680664063, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# loRA","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:20:58.022754Z","iopub.execute_input":"2024-03-23T19:20:58.023180Z","iopub.status.idle":"2024-03-23T19:21:11.862713Z","shell.execute_reply.started":"2024-03-23T19:20:58.023147Z","shell.execute_reply":"2024-03-23T19:21:11.861428Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.38.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.28.0)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.2)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.21.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.15.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\n# Define el modelo base\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"roberta-base\", num_labels=2, id2label=id2label, label2id=label2id\n)\n\n# Define la configuración de Lora\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n    target_modules=[\"self.query\", \"self.key\", \"self.value\", \"self.out\"],\n)\n\n# Crea el modelo PEFT\npeft_model = get_peft_model(model, config)\n\n# Define los argumentos de entrenamiento\ntraining_args = TrainingArguments(\n    output_dir=\"imdbreviews_classification_roberta_lora_v01\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=True,\n)\n\n# Define el entrenador\ntrainer = Trainer(\n    model=peft_model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Inicia el entrenamiento\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T19:22:58.707293Z","iopub.execute_input":"2024-03-23T19:22:58.707678Z","iopub.status.idle":"2024-03-23T20:13:37.732408Z","shell.execute_reply.started":"2024-03-23T19:22:58.707646Z","shell.execute_reply":"2024-03-23T20:13:37.730752Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240323_192312-ldo4b43h</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/expanaliticawandb/huggingface/runs/ldo4b43h' target=\"_blank\">treasured-gorge-4</a></strong> to <a href='https://wandb.ai/expanaliticawandb/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/expanaliticawandb/huggingface' target=\"_blank\">https://wandb.ai/expanaliticawandb/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/expanaliticawandb/huggingface/runs/ldo4b43h' target=\"_blank\">https://wandb.ai/expanaliticawandb/huggingface/runs/ldo4b43h</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 49:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.201800</td>\n      <td>0.190306</td>\n      <td>0.928800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.183800</td>\n      <td>0.175684</td>\n      <td>0.933700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=0.25303595352172853, metrics={'train_runtime': 3020.0978, 'train_samples_per_second': 21.191, 'train_steps_per_second': 0.662, 'total_flos': 1.7027336293995264e+16, 'train_loss': 0.25303595352172853, 'epoch': 2.0})"},"metadata":{}}]}]}